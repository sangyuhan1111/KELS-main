{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from models.ViT import ViT_LRP_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datapaths = ['./preprocessed/prepared/nan/L2Y1.pkl','./preprocessed/prepared/nan/L2Y2.pkl','./preprocessed/prepared/nan/L2Y3.pkl','./preprocessed/prepared/nan/L2Y4.pkl','./preprocessed/prepared/nan/L2Y5.pkl','./preprocessed/prepared/nan/L2Y6.pkl',]\n",
    "label_datapath = './preprocessed/prepared/nan/label.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickle\n",
    "input_datas = []\n",
    "for datapath in X_datapaths:\n",
    "    temp = pd.read_pickle(datapath)\n",
    "    temp = temp.reset_index()\n",
    "    input_datas.append(temp)\n",
    "label_data = pd.read_pickle(label_datapath)\n",
    "label_data = label_data.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "seq_len = len(input_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = label_data - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS2IDX = {\n",
    "    0 : '1등급',\n",
    "    1 : '2등급',\n",
    "    2 : '3등급',\n",
    "    3 : '4등급',\n",
    "    4 : '5등급',\n",
    "    5 : '6등급',\n",
    "    6 : '7등급',\n",
    "    7 : '8등급',\n",
    "    8 : '9등급'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testlist = []\n",
    "# for data in input_datas:\n",
    "#     #print(data.shape)\n",
    "#     #data.iloc[:,0]\n",
    "#     testlist.append(data.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.concat(testlist,axis=1)\n",
    "# t5 = test.iloc[:,5].isna().to_numpy()\n",
    "\n",
    "# test_t5 = test.iloc[t5]\n",
    "# t4 = test_t5.iloc[:,4].isna().to_numpy()\n",
    "# test_t4 = test_t5.iloc[t4]\n",
    "# t3 = test_t4.iloc[:,3].isna().to_numpy()\n",
    "# test_t3 = test_t4.iloc[t3]\n",
    "# test_t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,in_features,hidden_features=None,out_features=None,act_layer=nn.ReLU,dropout=0.1):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        self.fc1=nn.Linear(in_features,hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_embbedings(datas,networks):\n",
    "    \"\"\"\n",
    "    batches : years of data. seperated outputs of dataloader. each element of datas has different feature size.\n",
    "    network : embedding linear networks that matches feature size of each data\n",
    "    return list of embbeding and boolian matrix of nan\n",
    "\n",
    "    \"\"\"\n",
    "    emb_list = []\n",
    "    emb_nonnan_list = []\n",
    "    for i,net in enumerate(networks):\n",
    "        emb = net(datas[i])\n",
    "        emb_list.append(emb)\n",
    "        \n",
    "    \n",
    "    return emb_list\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_splited_data(input_datas,label_data,is_regression=False):\n",
    "\n",
    "    def apply_scaler(datain,scaler):\n",
    "        \n",
    "        fitted = scaler.fit(datain)\n",
    "        output = scaler.transform(datain)\n",
    "        output = pd.DataFrame(output,columns = datain.columns, index=list(datain.index.values))\n",
    "        return output\n",
    "\n",
    "    tup = train_test_split(input_datas[0],input_datas[1],input_datas[2],input_datas[3],input_datas[4],input_datas[5],label_data,train_size=0.8)\n",
    "    #input data에 따라 이쁘게 할 수 없나..\n",
    "    X_trains = []\n",
    "    X_tests = []\n",
    "    for i in range(len(input_datas)):\n",
    "        X_trains.append(tup[2*i].reset_index())\n",
    "        X_tests.append(tup[2*i+1].reset_index())\n",
    "    y_trains = [tup[-2].reset_index()]\n",
    "    y_tests = [tup[-1].reset_index()]\n",
    "    \n",
    "\n",
    "    for datas in X_trains, X_tests:\n",
    "        for i,data in enumerate(datas):\n",
    "            datas[i] = data.drop(columns=['level_0','index'])\n",
    "            min_max_scaler = MinMaxScaler()\n",
    "            datas[i] = apply_scaler(datas[i],min_max_scaler)\n",
    "\n",
    "    for datas in y_trains, y_tests:\n",
    "        for i,data in enumerate(datas):\n",
    "            datas[i] = data.drop(columns=['level_0','index'])\n",
    "            if is_regression == True:\n",
    "                min_max_scaler = MinMaxScaler()\n",
    "                datas[i] = apply_scaler(datas[i],min_max_scaler)\n",
    "\n",
    "    return X_trains, X_tests, y_trains[0], y_tests[0] # return list of sequences and a label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_regression = False\n",
    "X_trains, X_tests, y_train, y_test = make_splited_data(input_datas,label_data,is_regression=is_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_list(year_datas):\n",
    "    \"\"\"make split list used for spliting batches. batches must be splitted with torch.tensor_split with split_list\"\"\"\n",
    "    split_list = []\n",
    "    split = 0\n",
    "    for data in year_datas:\n",
    "        split += data.shape[1]\n",
    "        split_list.append(split)\n",
    "    split_list.pop() # \n",
    "    return split_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_splited_datas(batch,split_list):\n",
    "    list = torch.tensor_split(batch,split_list,dim=1)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KELSDataSet(Dataset):\n",
    "    def __init__(self,year_datas,label,is_regression=False):\n",
    "        \n",
    "        for i,data in enumerate(year_datas):\n",
    "            year_datas[i] = data.to_numpy()\n",
    "        self.split_list = make_split_list(year_datas) # used after getitem of dataloader.\n",
    "        self.is_regression = is_regression\n",
    "        self.label = label.to_numpy()\n",
    "        self.seq_len = len(year_datas)\n",
    "        self.data_len = year_datas[0].shape[0]\n",
    "        self.data = np.concatenate(year_datas,axis=1)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        x = torch.FloatTensor(self.data[idx])\n",
    "        if self.is_regression == True:\n",
    "            y_E,y_K,y_M  = torch.FloatTensor(self.label[idx])[0],torch.FloadTensor(self.label[idx])[1],torch.FloadTensor(self.label[idx])[2]\n",
    "        else:\n",
    "            y_E,y_K,y_M = torch.LongTensor(self.label[idx])[0],torch.LongTensor(self.label[idx])[1],torch.LongTensor(self.label[idx])[2]\n",
    "\n",
    "        return (x,(y_E,y_K,y_M))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KELSDataSet(X_trains,y_train,is_regression=is_regression)\n",
    "test_dataset = KELSDataSet(X_tests,y_test,is_regression=is_regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "hidden_features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embbed_dim = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embbeding_networks(train_dataset,batch_size=None,hidden_features = 100, out_features = 72, dropout=0.1):\n",
    "    \"\"\"make embedding networks based on train_dataset. batch size must be same with dataloaders.\"\"\"\n",
    "    split_list = train_dataset.split_list\n",
    "    sample_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    assert batch_size\n",
    "    (sample,label) = next(iter(sample_loader))\n",
    "    sample_datas = batch_to_splited_datas(sample,split_list)\n",
    "    embbeding_networks = [] # embbeding networks : 총 6개의 인코딩 네트워크. 흠.. nan 들어오면 batch x feature 사이즈의 nan true false 내놔야..?\n",
    "    # batch x seq 의 nanlist도 필요..\n",
    "    \n",
    "    for sample_data in sample_datas:\n",
    "        in_features = sample_data.shape[1]\n",
    "        emb_net = Embedding(in_features,hidden_features=hidden_features,out_features=out_features,dropout=dropout)\n",
    "        embbeding_networks.append(emb_net)\n",
    "    return embbeding_networks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embbeding_networks = make_embbeding_networks(train_dataset,batch_size=batch_size,hidden_features = hidden_features,out_features=embbed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "split_list = train_dataset.split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#def train_net\n",
    "model_E = ViT_LRP_copy.VisionTransformer(seq_len=6, num_classes=9, embed_dim=72, depth=8,\n",
    "                 num_heads=6, mlp_ratio=4., qkv_bias=False, mlp_head=False, drop_rate=0.1, attn_drop_rate=0.1)\n",
    "model_K = ViT_LRP_copy.VisionTransformer(seq_len=6, num_classes=9, embed_dim=72, depth=8,\n",
    "                 num_heads=6, mlp_ratio=4., qkv_bias=False, mlp_head=False, drop_rate=0.1, attn_drop_rate=0.1)\n",
    "model_M = ViT_LRP_copy.VisionTransformer(seq_len=6, num_classes=9, embed_dim=72, depth=8,\n",
    "                 num_heads=6, mlp_ratio=4., qkv_bias=False, mlp_head=False, drop_rate=0.1, attn_drop_rate=0.1)                 \n",
    "for concated_data,(label_E,label_K,label_M) in train_loader:\n",
    "    datas = batch_to_splited_datas(concated_data,split_list)\n",
    "    emb_batch_list= batch_to_embbedings(datas,embbeding_networks) # can be used for contrastive loss\n",
    "    # emb_batch_list : 임베딩 벡터들의 리스트. 얘를 이제 batch x seq x feature 행렬로 쌓음\n",
    "    emb_batched_seq = torch.stack(emb_batch_list).transpose(0,1)\n",
    "    attn_mask = make_attn_mask(emb_batched_seq)\n",
    "    (E_score,K_score,M_score) = model_E(emb_batched_seq,attn_mask),model_K(emb_batched_seq,attn_mask),model_M(emb_batched_seq,attn_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_E = criterion(E_score,label_E)\n",
    "loss_K = criterion(K_score,label_K)\n",
    "loss_M = criterion(M_score,label_M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 6, 72])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_batched_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_attn_mask(emb_seq_batch):\n",
    "    \"\"\"\n",
    "    make attention mask from embedding batch. \n",
    "    batch = (batch, seq_len,embedding_size)\n",
    "    \"\"\"\n",
    "    batch_size = emb_seq_batch.shape[0]\n",
    "    emb_seq_batch_isnan = torch.isnan(emb_seq_batch)\n",
    "    emb_seq_batch[emb_seq_batch_isnan] = 0\n",
    "\n",
    "\n",
    "    attn_mask = emb_seq_batch_isnan[:,:,0]\n",
    "    temp = torch.BoolTensor(batch_size)\n",
    "\n",
    "    temp[:] = False\n",
    "    attn_mask = torch.concat((temp.unsqueeze(1),attn_mask),dim=1)\n",
    "\n",
    "    attn_mask = attn_mask.unsqueeze(1).expand(-1,seq_len+1,-1)\n",
    "    attn_mask = attn_mask.unsqueeze(1)\n",
    "    return attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6, 5218]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5w/ftdjr53s4wdbntzkf32v2hl80000gn/T/ipykernel_10893/2971263441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_datas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/python_3.9/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2417\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python_3.9/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \"\"\"\n\u001b[1;32m    369\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python_3.9/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6, 5218]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#make them into Dataloader, __getitem__ should return list of (Batch, feature_size ). 리스트 안 개수 : 6개.\n",
    "# \n",
    "\n",
    "# define 6 of embbeding network. each network must be built\n",
    "#\n",
    "# select a one batch sample.\n",
    "# for i,sample in enumarate sample_list : \n",
    "#       emb_net[i] = MLP(in_feature,hidden_feature,out_feature. there must be dropout.)\n",
    "# \n",
    "\n",
    "# make nanlist for batch. (B,nanlist) 꼴. nanlist는 sequence_length와 매치\n",
    "# after embedding, input should be (Batch,Sequence_length,feature size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defince training and evaluation."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef200163ac0f4291da233fe0d4efaa9bca84b8d4b04fcfb449644928c6966e7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('python_3.9': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
